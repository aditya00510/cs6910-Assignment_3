{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Assignment3_Q1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jCepbuIyKU-",
        "outputId": "4f48d7e2-7fa4-4dc7-eb04-94dc7864eb4f"
      },
      "source": [
        "!curl --header \"Host: storage.googleapis.com\" --header \"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.128 Safari/537.36 Edg/89.0.774.77\" --header \"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header \"Accept-Language: en-US,en;q=0.9\" --header \"Referer: https://github.com/google-research-datasets/dakshina\" \"https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\" -L -o \"dakshina_dataset_v1.0.tar\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1915M  100 1915M    0     0  72.3M      0  0:00:26  0:00:26 --:--:-- 75.9M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45-MBmWkyRO1"
      },
      "source": [
        "import shutil\n",
        "shutil.unpack_archive(\"/content/dakshina_dataset_v1.0.tar\",'/content/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bTZURNJyRDy"
      },
      "source": [
        "import re\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential,Model,load_model\n",
        "from keras.layers import Dense,LSTM,GRU,SimpleRNN,Input,Dropout,TimeDistributed,RepeatVector,dot,BatchNormalization,concatenate,multiply,Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RsZx8CoyRAX"
      },
      "source": [
        "class seq2seq:\n",
        "    def __init__(self,cell,embedding_size,latent_dim,encoder_layers_size,decoder_layers_size,dropouts,epochs,batch_size):\n",
        "        self.cell = cell\n",
        "        self.embedding_size = embedding_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.encoder_layers_size = encoder_layers_size\n",
        "        self.decoder_layers_size = decoder_layers_size\n",
        "        self.dropouts=dropouts\n",
        "        self.epochs=epochs\n",
        "        self.batch_size=batch_size\n",
        "\n",
        "    def get_data(self,path):\n",
        "        d = pd.read_csv(path,sep=\"\\t\",header=None,error_bad_lines=False)\n",
        "        d = d.dropna()\n",
        "\n",
        "        decoder_target_data = np.zeros((d.shape[0],self.max_length_y,self.decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "        for i,target_text in enumerate(d[0]):\n",
        "            target_text = '\\t'+target_text+'\\n'\n",
        "            for t, char in enumerate(target_text):\n",
        "                if t > 0:\n",
        "                    decoder_target_data[i, t - 1, self.target_token_index[char]] = 1.0\n",
        "            decoder_target_data[i, t:, self.target_token_index[\"\\n\"]] = 1.0\n",
        "\n",
        "        return ([[self.input_token_index[letter] for letter in list('\\t'+word+'\\n')] for word in d[1]]),\\\n",
        "                ([[self.target_token_index[letter] for letter in list('\\t'+word+'\\n')] for word in d[0]]),decoder_target_data\n",
        "\n",
        "    def create_vocab(self,path):\n",
        "        d = pd.read_csv(path,sep=\"\\t\",header=None,error_bad_lines=False)\n",
        "        d = d.dropna()\n",
        "\n",
        "        x = [list('\\t'+word+'\\n') for word in np.array(d[1])]\n",
        "        y = [list('\\t'+word+'\\n') for word in np.array(d[0])]\n",
        "\n",
        "        telugu_vocab = set()\n",
        "        english_vocab = set()\n",
        "\n",
        "        for word in x:\n",
        "            for char in word:\n",
        "                english_vocab.add(char)\n",
        "\n",
        "        for word in y:\n",
        "            for char in word:\n",
        "                telugu_vocab.add(char)\n",
        "\n",
        "        telugu_list = sorted(list(telugu_vocab))\n",
        "        english_list = sorted(list(english_vocab))\n",
        "\n",
        "        max_length_x = (np.max([len(i) for i in x]))\n",
        "        max_length_y = (np.max([len(i) for i in y]))\n",
        "\n",
        "        return telugu_list,english_list,max_length_x,max_length_y    \n",
        "\n",
        "    def create_data(self):\n",
        "        train_path = \"/content/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\"\n",
        "        cv_path = \"/content/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\"\n",
        "        test_path = \"/content/dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv\"\n",
        "\n",
        "        telugu_list,english_list,self.max_length_x,self.max_length_y = self.create_vocab(train_path)\n",
        "        self.encoder_tokens = len(english_list)\n",
        "        self.decoder_tokens = len(telugu_list)\n",
        "\n",
        "        # Dict for char to index\n",
        "        self.input_token_index = dict([(char, i) for i, char in enumerate(english_list)])\n",
        "        self.target_token_index = dict([(char, i) for i, char in enumerate(telugu_list)])\n",
        "\n",
        "        # Dict for index to char\n",
        "        self.inv_input_token_index = dict({(value,key) for key,value in self.input_token_index.items()})\n",
        "        self.inv_target_token_index = dict({(value,key) for key,value in self.target_token_index.items()})\n",
        "\n",
        "        encoder_train,decoder_train,self.decoder_target_train = self.get_data(train_path)\n",
        "        encoder_cv,decoder_cv,self.decoder_target_cv = self.get_data(cv_path)\n",
        "        encoder_test,decoder_test,self.decoder_target_test = self.get_data(test_path)\n",
        "\n",
        "\n",
        "        self.encoder_train = sequence.pad_sequences(encoder_train,maxlen=self.max_length_x,padding=\"post\")\n",
        "        self.decoder_train = sequence.pad_sequences(decoder_train,maxlen=self.max_length_y,padding=\"post\")\n",
        "        self.encoder_cv = sequence.pad_sequences(encoder_cv,maxlen=self.max_length_x,padding=\"post\")\n",
        "        self.decoder_cv = sequence.pad_sequences(decoder_cv,maxlen=self.max_length_y,padding=\"post\")\n",
        "        self.encoder_test = sequence.pad_sequences(encoder_test,maxlen=self.max_length_x,padding=\"post\")\n",
        "        self.decoder_test = sequence.pad_sequences(decoder_test,maxlen=self.max_length_y,padding=\"post\")\n",
        "\n",
        "    def create_model(self):\n",
        "        encoder_inputs = Input(shape=(None,))\n",
        "        x = Embedding(self.encoder_tokens,self.embedding_size,input_length=self.max_length_x)(encoder_inputs)\n",
        "        for _ in range(self.encoder_layers_size):\n",
        "            if self.cell == \"lstm\":\n",
        "                x,state_h,state_c = LSTM(self.latent_dim,return_state=True,return_sequences=True,dropout=self.dropouts)(x)\n",
        "                encoder_states = [state_h,state_c]\n",
        "            elif self.cell == \"rnn\":\n",
        "                x,state_c = SimpleRNN(self.latent_dim,return_state=True,return_sequences=True,dropout=self.dropouts)(x)\n",
        "                encoder_states = [state_c]\n",
        "            elif self.cell == \"gru\":\n",
        "                x,state_c = GRU(self.latent_dim,return_state=True,return_sequences=True,dropout=self.dropouts)(x)\n",
        "                encoder_states = [state_c]\n",
        "                        \n",
        "\n",
        "        decoder_inputs = Input(shape=(None,))\n",
        "        decoder_embb = Embedding(self.decoder_tokens,self.embedding_size,input_length=self.max_length_y)(decoder_inputs)\n",
        "        if self.cell == \"lstm\":\n",
        "            decoder_lstm,state_h,state_c = LSTM(self.latent_dim, return_sequences=True,return_state=True,dropout=self.dropouts)(decoder_embb,initial_state=encoder_states)\n",
        "        elif self.cell == \"rnn\":\n",
        "            decoder_lstm,state_h = SimpleRNN(self.latent_dim, return_sequences=True,return_state=True,dropout=self.dropouts)(decoder_embb,initial_state=encoder_states)\n",
        "        elif self.cell == \"gru\":\n",
        "            decoder_lstm,state_h = GRU(self.latent_dim, return_sequences=True,return_state=True,dropout=self.dropouts)(decoder_embb,initial_state=encoder_states)\n",
        "\n",
        "        for i in range(self.decoder_layers_size-1):\n",
        "            if self.cell == \"lstm\":\n",
        "                decoder_lstm,state_h,state_c = LSTM(self.latent_dim, return_sequences=True,return_state=True,dropout=self.dropouts)(decoder_lstm)\n",
        "            elif self.cell == \"rnn\":\n",
        "                decoder_lstm,state_h = SimpleRNN(self.latent_dim, return_sequences=True,return_state=True,dropout=self.dropouts)(decoder_lstm)\n",
        "            elif self.cell == \"gru\":\n",
        "                decoder_lstm,state_h = GRU(self.latent_dim, return_sequences=True,return_state=True,dropout=self.dropouts)(decoder_lstm)\n",
        "        \n",
        "\n",
        "        decoder_outputs = TimeDistributed(Dense(self.decoder_tokens, activation='softmax'))(decoder_lstm)\n",
        "\n",
        "        model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def percentage_of_correct_test_predictions(self,model):\n",
        "        count = 0\n",
        "        for i in tqdm(range(self.encoder_test.shape[0])):\n",
        "            pred = model([np.expand_dims(self.encoder_test[i],0),np.expand_dims(self.decoder_test[i],0)])\n",
        "            actual_out = re.sub('\\t|\\n','',''.join([self.inv_target_token_index[self.decoder_test[i][j]] for j in range(self.max_length_y)]))\n",
        "            predicted_out = re.sub('\\t|\\n','',''.join(self.inv_target_token_index[np.argmax(pred[0][j])] for j in range(self.max_length_y)))\n",
        "            if (actual_out==predicted_out):\n",
        "                count+=1\n",
        "        return count/self.encoder_test.shape[0]\n",
        "\n",
        "    def run(self,model):\n",
        "        # Compile & run training\n",
        "        model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "\n",
        "        model.fit([self.encoder_train,self.decoder_train], self.decoder_target_train,\n",
        "                self.batch_size,\n",
        "                self.epochs,validation_data=([self.encoder_cv,self.decoder_cv], self.decoder_target_cv))\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFkSbsO6ykg-",
        "outputId": "6ea641bd-5251-42d1-f6e9-8d4b655f34e1"
      },
      "source": [
        "s2s=seq2seq(\"rnn\",12,512,2,2,0.2,3,32)\n",
        "s2s.create_data()\n",
        "model=s2s.create_model()\n",
        "s2s.run(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1830/1830 [==============================] - 135s 71ms/step - loss: 1.5403 - accuracy: 0.6478 - val_loss: 1.0065 - val_accuracy: 0.7223\n",
            "Epoch 2/3\n",
            "1830/1830 [==============================] - 128s 70ms/step - loss: 1.0590 - accuracy: 0.7004 - val_loss: 0.9629 - val_accuracy: 0.7324\n",
            "Epoch 3/3\n",
            "1830/1830 [==============================] - 129s 70ms/step - loss: 1.0053 - accuracy: 0.7150 - val_loss: 0.9326 - val_accuracy: 0.7406\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoC-oh7XzWON",
        "outputId": "b16bbce0-036f-44a2-ef12-1d5f83dacc46"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, None, 12)     336         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "simple_rnn_4 (SimpleRNN)        [(None, None, 512),  268800      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, None, 12)     780         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "simple_rnn_5 (SimpleRNN)        [(None, None, 512),  524800      simple_rnn_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "simple_rnn_6 (SimpleRNN)        [(None, None, 512),  268800      embedding_3[0][0]                \n",
            "                                                                 simple_rnn_5[0][1]               \n",
            "__________________________________________________________________________________________________\n",
            "simple_rnn_7 (SimpleRNN)        [(None, None, 512),  524800      simple_rnn_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, None, 65)     33345       simple_rnn_7[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 1,621,661\n",
            "Trainable params: 1,621,661\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}